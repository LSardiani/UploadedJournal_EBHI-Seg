# -*- coding: utf-8 -*-
"""Group B_J_EBHI_Seg_Final_Project(AttUNet).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/LSardiani/UploadedJournal_EBHI-Seg/blob/main/Group%20B_J_EBHI_Seg_Final_Project(AttUNet).ipynb

## Lily Silva Ardiani - ADA LOVELACE Group

# This was our Final Project from Indonesia AI Computer Vision Bootcamp 2

We used **Enteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset for Image Segmentation Tasks (EBHI-Seg)**

 The Dataset is available for public, and downloaded from https://figshare.com/articles/dataset/EBHISEG/21540159/1

 Shi L, Li X, Hu W, Chen H, Chen J, Fan Z, Gao M, Jing Y, Lu G, Ma D, Ma Z, Meng Q, Tang D, Sun H, Grzegorzek M, Qi S, Teng Y, Li C. EBHI-Seg: A novel enteroscope biopsy histopathological hematoxylin and eosin image dataset for image segmentation tasks. Front Med (Lausanne). 2023 Jan 24;10:1114673. doi: 10.3389/fmed.2023.1114673. PMID: 36760405; PMCID: PMC9902656.

# Environment Setup
"""

!pip install pytorch-lightning

!pip install torchinfo

!pip install vision_transformer_pytorch

!pip install vit-pytorch
!pip install graphviz
!pip install torchview
!pip install torchviz

import pandas as pd
import numpy as np
import os
import json
import cv2
import pytorch_lightning as pl
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.utils as utils
import torchvision.transforms.functional as tf
import torch.nn.functional as f
import torchvision
import torchmetrics as metrics
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import LearningRateMonitor
from PIL import Image, ImageColor
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torchinfo import summary
from torch.utils.data import DataLoader, Dataset, random_split
from sklearn.model_selection import train_test_split
from pathlib import Path
#from models.unetArch import UNet, dice_loss, multiclass_dice_coef
from torchmetrics.functional.classification import multilabel_precision, multilabel_recall
import random
from torchviz import make_dot, make_dot_from_trace

from google.colab import drive
drive.mount('/content/drive')

"""# Load Dataset"""

import zipfile

zip_path = '/content/drive/MyDrive/EBHI-SEG.zip'
destination_folder = '/content/drive/MyDrive/EBHI-SEG'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall()

colors = [
    (255, 255, 255), # Background (white)
    (0, 0, 255),    # Normal (blue)
    (51, 255, 153),  # Polyp (cyanic green)
    (204, 255, 51),  # Serrated adenoma (yelowish green)
    (255, 255, 0),  # Low-grade IN (yellow)
    (255, 153, 0), # High-grade IN (orange)
    (204, 51, 0), # Adenocarcinoma (red)
]

"""# Data Preparation System"""

trainDf = pd.read_csv('traindata.csv')
valDf = pd.read_csv('valdata.csv')
testDf = pd.read_csv('testdata.csv')

trainDf['Path'] = trainDf['Path'].str.strip()

trainDf

dataDir = Path('EBHI-SEG')

classNames = [classes for classes in os.listdir(dataDir) if os.path.isdir(os.path.join(dataDir, classes))]
imageList = []
labelList = []
classList = []
splitClass = []
trainList = trainDf['Path'].tolist()
valList = valDf['Path'].tolist()
testList = testDf['Path'].tolist()

for className in classNames:
    classDir = os.path.join(dataDir, className)
    imageDir = os.path.join(classDir, 'image')
    labelDir = os.path.join(classDir, 'label')
    imagePath = [file for file in os.listdir(imageDir)]
    labelPath = [file for file in os.listdir(labelDir)]

    for img in imagePath:
        if img in labelPath:
            imageList.append(os.path.join(imageDir, img))
            labelList.append(os.path.join(labelDir, img))
            classList.append(className)
            if os.path.join(imageDir, img) in trainList:
                splitClass.append('train')
            elif os.path.join(imageDir, img) in valList:
                splitClass.append('validation')
            else:
                splitClass.append('testing')

df = pd.DataFrame({'image' : imageList, 'label' : labelList, 'class' : classList, 'split' : splitClass})

df['split'].unique()

for folder in dataDir.iterdir():
    if folder.is_dir():
        for folder2 in folder.iterdir():
            if folder2.is_dir():
                print(folder.name)
                print(folder2.name)
                print(len(os.listdir(folder2)))

df.to_csv('imageLabel.csv', index=False)

"""# Data preparation"""

df = pd.read_csv('imageLabel.csv')
df

df[df['split'] == 'train']

df[df['split'] == 'validation']

classes = {
    'Normal' : 1,
    'Polyp' : 2,
    'Low-grade IN' : 4,
    'High-grade IN' : 5,
    'Serrated adenoma' : 3,
    'Adenocarcinoma' : 6
}

randidx = np.random.randint(0, len(df))
# Show one sample image
colorMap = np.array(colors, dtype=np.uint8)
img = Image.open(df.iloc[randidx,0])
lbl = np.array(Image.open(df.iloc[randidx,1]))
lblBin = (lbl > 0).astype(np.uint8) * classes[df.iloc[randidx,2]]
coloredBin = Image.fromarray(colorMap[lblBin]).convert('RGB')
overlay = Image.blend(img, coloredBin, alpha=.6)
overlay

df['class'].unique()

class customDataset(Dataset):
    def __init__(self, imageList, labelList, classList, classDict, transforms=None):
        assert len(imageList) == len(labelList) and len(imageList) == len(classList) and len(labelList) == len(classList), '3 of the list are not the same length'
        self.imageList = imageList
        self.labelList = labelList
        self.classList = classList
        self.classDict = classDict
        self.transforms = transforms
    def __len__(self):
        return len(self.imageList)
    def __getitem__(self, index):
        image = np.array(Image.open(self.imageList[index]))
        mask = np.array(Image.open(self.labelList[index]))
        mask = (mask > 0).astype(np.uint8) * self.classDict[self.classList[index]]
        if self.transforms:
            transformed = self.transforms(image=image, mask=mask)
            imgAug = transformed['image'].contiguous()
            maskAug = transformed['mask'].contiguous()
            return imgAug, maskAug
        image = torch.as_tensor(image).float().contiguous()
        mask = torch.as_tensor(mask).long().contiguous()
        return image, mask

class dataModule(pl.LightningDataModule):
    def __init__(self, df, classDict, batchSize):
        super().__init__()
        self.df = df
        self.batchSize = batchSize
        self.classDict = classDict
    def prepare_data(self):
        #No data download, so we pass this
        pass
    def _getTransform(self,train=True):
        if train:
            return A.Compose([
                A.OneOf([
                    A.VerticalFlip(),
                    A.HorizontalFlip(),
                    A.RandomRotate90(),
                ], p=.5),
                ToTensorV2()
            ])
        return A.Compose([
            ToTensorV2()
        ])
    def setup(self, stage: str):
        trainTransform = self._getTransform()
        valTransform = self._getTransform(False)
        self.trainingSet = customDataset(df[df['split'] == 'train']['image'].tolist(), df[df['split'] == 'train']['label'].tolist(), df[df['split'] == 'train']['class'].tolist(), classDict=self.classDict, transforms=trainTransform)
        self.valSet = customDataset(df[df['split'] == 'validation']['image'].tolist(), df[df['split'] == 'validation']['label'].tolist(), df[df['split'] == 'validation']['class'].tolist(), classDict=self.classDict, transforms=valTransform)
        self.testSet = customDataset(df[df['split'] == 'testing']['image'].tolist(), df[df['split'] == 'testing']['label'].tolist(), df[df['split'] == 'testing']['class'].tolist(), classDict=self.classDict, transforms=valTransform)
    def train_dataloader(self):
        return DataLoader(self.trainingSet, batch_size=self.batchSize, num_workers=4, drop_last=True, pin_memory=True, shuffle=True)
    def val_dataloader(self):
        return DataLoader(self.valSet, batch_size=self.batchSize, num_workers=4, drop_last=True, pin_memory=True, shuffle=False)
    def predict_dataloader(self):
        return DataLoader(self.testSet, batch_size=self.batchSize, num_workers=4, drop_last=True, pin_memory=True, shuffle=False)

import torch
from torch import Tensor

# Define function to calculate DICE Coefficient between two tensors of input and target

def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):
    # Make sure the same size between input and target
    assert input.size() == target.size()
    # Make sure input is 3D tensor or do not reduce batch dimension if not allowed
    assert input.dim() == 3 or not reduce_batch_first

    # Calculate dimension for the DICE
    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)

    # Calculate intersection
    inter = 2 * (input * target).sum(dim=sum_dim)

    # Calculate positive element in input and target
    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)

    # Avoid division by zero with changing zero in sets_num by inter
    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)

    # Calculate DICE coefficient by adding epsilon to avoid division by zero
    dice = (inter + epsilon) / (sets_sum + epsilon)

    # Return mean DICE coefficcient
    return dice.mean()

# Define function to calculate DICE coefficient in multiclass between two tensors of input and target

def multiclass_dice_coef(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):
    # Use DICE coefficient after flatten input and target tensors
    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)


# Define function to calculate DICE loss in multiclass

def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):
    # Choose the function based on multiclass activation
    fn = multiclass_dice_coef if multiclass else dice_coeff
    # DICE loss is 1-DICE coefficient
    return 1 - fn(input, target, reduce_batch_first=True)

"""# AttUNet"""

import torch
import torch.nn as nn
import torch.nn.functional as F

import math

class SpatialAttention(nn.Module):
    def __init__(self):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size=3, padding=1)

    def forward(self, x):
        avg = torch.mean(x, dim=1, keepdim=True)
        max_val, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat((avg, max_val), dim=1)  # Concatenate along the channel dimension
        x = self.conv(x)
        return torch.sigmoid(x)

class ChannelAttention(nn.Module):
    def __init__(self, in_channels):
        super(ChannelAttention, self).__init__()
        self.in_channel = in_channels
        self.convblock = nn.Sequential(
            nn.Conv2d(in_channels, math.ceil(in_channels // 16), 1),
            nn.Conv2d(math.ceil(in_channels // 16), math.ceil(in_channels // 16), 3, padding=1),
            nn.Conv2d(math.ceil(in_channels // 16), in_channels, 1)
        )

    def forward(self, x):
        avg = torch.mean(x, dim=(-2,-1), keepdim=True)
        max_val, _ = torch.max(x.view(x.size(0), x.size(1), -1), dim=2, keepdim=True)
        max_val = max_val.view(x.size(0), x.size(1), 1, 1)
        outAvg = self.convblock(avg)
        outMax = self.convblock(max_val)
        x = torch.add(outAvg, outMax)
        return torch.sigmoid(x)

class AttentionConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, residual=False):
        super(AttentionConvBlock, self).__init__()
        self.residual = residual
        self.resconv = nn.Conv2d(in_channels, out_channels, 1, 1)
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
        self.spatialAtt = SpatialAttention()
        self.channelAtt = ChannelAttention(out_channels)

    def forward(self, x):
        if self.residual:
            residual = self.resconv(x)
        x = self.double_conv(x)
        cam = self.channelAtt(x)
        sam = self.spatialAtt(x)
        camx = torch.mul(x, cam)
        samx = torch.mul(x, sam)
        if self.residual:
            return  camx + samx + residual
        return camx + samx

class AttDown(nn.Module):
    def __init__(self, in_channels, out_channels, residual=False, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.maxpool_conv = nn.Sequential(
            # Max-pooling for downsampling
            nn.MaxPool2d(2),
            # Two layers conv
            AttentionConvBlock(in_channels, out_channels, residual=residual)
        )

    def forward(self, x):
        return self.maxpool_conv(x)


class AttUp(nn.Module):
    def __init__(self, in_channels, out_channels, bilinear=True, residual=False,*args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        if bilinear:
            # Upsampling by bilinear interpolation
            self.up = nn.Upsample(
                scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = AttentionConvBlock(in_channels, out_channels // 2, residual=residual)
        else:
            # Upsampling by transpose conv
            self.up = nn.ConvTranspose2d(
                in_channels, in_channels // 2, kernel_size=2, stride=2)
            self.conv = AttentionConvBlock(in_channels, out_channels, residual=residual)

    def forward(self, x1, x2):
        x1 = self.up(x1)
        diffY = x2.size()[2] - x1.size()[2]
        diffX = x2.size()[3] - x1.size()[3]

        # Padding to change size x1 = x2
        x1 = F.pad(x1, [diffX // 2, diffX - diffX //
                   2, diffY // 2, diffY - diffY // 2])

        # Concatenate x1 and x2, then two layers conv
        x = torch.cat([x2, x1], dim=1)
        return self.conv(x)


class OutConv(nn.Module):
    def __init__(self, in_channels, out_channels, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        # Conv layer 1x1 for the output
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        return self.conv(x)

# Example usage:
if __name__ == "__main__":
    input_tensor = torch.randn(1, 3, 128, 128)  # 1 batch, 3 channels, 128x128 image
    model = AttentionConvBlock(3, 64, residual=True)
    output = model(input_tensor)
    print(output.shape)  # Should be (1, 64, 128, 128)
    down = AttDown(64, 128, residual=True)
    output2 = down(output)
    print(output2.shape)
    up = AttUp(128, 64, bilinear=False,residual=True)
    print(up(output2, output).size())

from torchinfo import summary

class AttUNetModel(nn.Module):
    def __init__(self, n_channels, n_classes, init_filter=64, bilinear=False, residual=False,*args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.n_channels = n_channels
        self.n_classes = n_classes
        self.bilinear = bilinear

        # AttUNet composition
        self.inputTensor = (AttentionConvBlock(n_channels, init_filter, residual=residual))
        self.down1 = (AttDown(init_filter, init_filter*2, residual=residual))
        self.down2 = (AttDown(init_filter*2, init_filter*4, residual=residual))
        self.down3 = (AttDown(init_filter*4, init_filter*8, residual=residual))
        factor = 2 if bilinear else 1
        self.down4 = (AttDown(init_filter*8, init_filter*16 // factor, residual=residual))
        self.up1 = (AttUp(init_filter*16, init_filter*8 // factor, bilinear, residual=residual))
        self.up2 = (AttUp(init_filter*8, init_filter*4 // factor, bilinear, residual=residual))
        self.up3 = (AttUp(init_filter*4, init_filter*2 // factor, bilinear, residual=residual))
        self.up4 = (AttUp(init_filter*2, init_filter, bilinear, residual=residual))
        self.outputTensor = (OutConv(init_filter, n_classes))

    def forward(self, x):
        x1 = self.inputTensor(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)
        x4 = self.down3(x3)
        x5 = self.down4(x4)
        x = self.up1(x5, x4)
        x = self.up2(x, x3)
        x = self.up3(x, x2)
        x = self.up4(x, x1)
        logits = self.outputTensor(x)
        return logits

    def use_checkpointing(self):
        # Checkpoints in some layers
        self.inputTensor = torch.utils.checkpoint(self.inputTensor)
        self.down1 = torch.utils.checkpoint(self.down1)
        self.down2 = torch.utils.checkpoint(self.down2)
        self.down3 = torch.utils.checkpoint(self.down3)
        self.down4 = torch.utils.checkpoint(self.down4)
        self.up1 = torch.utils.checkpoint(self.up1)
        self.up2 = torch.utils.checkpoint(self.up2)
        self.up3 = torch.utils.checkpoint(self.up3)
        self.up4 = torch.utils.checkpoint(self.up4)
        self.outputTensor = torch.utils.checkpoint(self.outputTensor)

if __name__ == "__main__":
    # Instantiate the model and display the summary
    model = AttUNetModel(n_channels=3, n_classes=7, init_filter=64)
    summary(model, input_size=(1,3,400,400))
    resmodel = AttUNetModel(n_channels=3, n_classes=7, init_filter=64, residual=True)
    summary(resmodel, input_size=(1,3,400,400))

model = AttUNetModel(n_channels=3, n_classes=5)
summary(model, input_size=(1,3,224,224))

x = torch.randn(1,3,224,224)
y = AttUNetModel(n_channels=3, n_classes=5)(x)

make_dot(y.mean(), params=dict(AttUNetModel(n_channels=3, n_classes=5).named_parameters()),
         show_attrs=True, show_saved=True).render("AttUNet", format="png")

class AttUNetModule(pl.LightningModule):
    def __init__(self, nClasses = 7, lr= 0.01, decay=0):
        super().__init__()
        self.model = AttUNetModel(n_channels=3, n_classes=nClasses)
        self.nClasses = nClasses
        self.lr = lr
        self.decay = decay
        self.save_hyperparameters()
        self.criterion = nn.CrossEntropyLoss()
    def forward(self, x):
        logits = self.model(x)
        return logits
    def training_step(self, batch, batch_idx):
        img, mask = batch
        img = img.to(device=self.device, dtype=torch.float32, memory_format=torch.channels_last)
        mask = mask.to(device=self.device, dtype=torch.long)
        predMask = self.forward(img)
        loss = self.criterion(predMask, mask)
        loss += dice_loss(f.softmax(predMask, dim=1).float(), f.one_hot(mask, self.nClasses).permute(0, 3, 1, 2).float(), multiclass=True)
        self.log('diceLoss', loss, on_step=False, on_epoch=True, prog_bar=True)
        return loss
    def validation_step(self, batch, batch_idx):
        img, mask = batch
        totalN = self.trainer.num_val_batches[0]
        img = img.to(device=self.device, dtype=torch.float32, memory_format=torch.channels_last)
        mask = mask.to(device=self.device, dtype=torch.long)
        assert mask.min() >= 0 and mask.max() < self.nClasses, f'True mask indices should be in [0, {self.nClasses}]'
        predMask = self.forward(img)
        mask = f.one_hot(mask, self.nClasses).permute(0, 3, 1, 2).float()
        predMask = f.one_hot(predMask.argmax(dim=1), self.nClasses).permute(0, 3, 1, 2).float()
        diceScore = multiclass_dice_coef(predMask[:,1:], mask[:, 1:], reduce_batch_first=False)
        self.log('diceScore', diceScore, on_step=False, on_epoch=True, prog_bar=True, reduce_fx=torch.mean)
        return diceScore
    def predict_step(self, batch, batch_idx):
        img, mask = batch
        img = img.to(device =self.device, dtype=torch.float32)
        outputs = self.forward(img)
        predMask = outputs.argmax(dim=1)

        mask = f.one_hot(mask.long(), self.nClasses).permute(0, 3, 1, 2).float()
        predictionMask = f.one_hot(predMask.cpu(), self.nClasses).permute(0, 3, 1, 2).float()

        return img, predMask.long().squeeze().cpu().numpy(), mask, predictionMask
    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, mode='min')
        return {
            'optimizer' : optimizer,
            'lr_scheduler' : {
                'scheduler' : scheduler,
                'monitor' : 'diceLoss'
            }
        }
        # return optimizer

"""# Train the Model"""

dataMod = dataModule(df=df, classDict=classes, batchSize=16)
model = AttUNetModule(lr=0.00005)
lrMonitor = LearningRateMonitor(logging_interval='epoch')
logger = TensorBoardLogger('tb_logs', name='AttUNetSegm')
trainer = pl.Trainer(accelerator='gpu', devices=[0], max_epochs=200, logger=logger, callbacks=[lrMonitor])
trainer.fit(model=model, datamodule=dataMod)

"""# Results"""

# Predict only
dataMod1 = dataModule(df=df, classDict=classes, batchSize=16)
model1 = AttUNetModule(lr=0.001)
lrMonitor1 = LearningRateMonitor(logging_interval='epoch')
# logger1 = TensorBoardLogger('tb_logs', name='AttUNetSegm')
trainer1 = pl.Trainer(accelerator='gpu', devices=[0], max_epochs=200, callbacks=[lrMonitor1])
results = trainer1.predict(model=model1, datamodule=dataMod1, ckpt_path='/content/tb_logs/AttUNetSegm/version_0/checkpoints/epoch=199-step=19400.ckpt')

results = trainer.predict(model=model, datamodule=dataMod)

numClasses = [0,1,2,3,4,5,6]

def mask_to_image(preds, mask_values, colors=colors):
    listOverlay = []

    originalMask = [batch[2] for batch in preds]
    originalMask = torch.stack([orMask for batch in originalMask for orMask in batch])
    predictionMask = [batch[3] for batch in preds]
    predictionMask = torch.stack([preMask for batch in predictionMask for preMask in batch])
    images = [batch[0] for batch in preds]
    images = torch.stack([img for batch in images for img in batch])
    masks = [batch[1] for batch in preds]
    masks = np.array([mask for batch in masks for mask in batch])
    recalls = multilabel_recall(predictionMask, originalMask, num_labels=len(mask_values), average='none')
    precisions = multilabel_precision(predictionMask, originalMask, num_labels=len(mask_values), average='none')
    print(f'recalls = background : {recalls[0]:.2f}, normal : {recalls[1]:.2f}, polyp : {recalls[2]:.2f}, serrated adenoma : {recalls[3]:.2f}, low-grade IN : {recalls[4]:.2f}, high-grade IN : {recalls[5]:.2f}, adenocarcinoma : {recalls[6]:.2f}')
    print(f'precisions = background : {precisions[0]:.2f}, normal : {precisions[1]:.2f}, polyp : {precisions[2]:.2f}, serrated adenoma : {precisions[3]:.2f}, low-grade IN : {precisions[4]:.2f}, high-grade IN : {precisions[5]:.2f}, adenocarcinoma : {precisions[6]:.2f}')
    for img, mask in zip(images, masks):
        if isinstance(mask_values[0], list):
            out = np.zeros((mask.shape[-2], mask.shape[-1], len(mask_values[0])), dtype=np.uint8)
        elif mask_values == [0, 1]:
            out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=bool)
        else:
            out = np.zeros((mask.shape[-2], mask.shape[-1]), dtype=np.uint8)

        if mask.ndim == 3:
            mask = np.argmax(mask, axis=0)

        for i, v in enumerate(mask_values):
            out[mask == i] = v

        classColorMap = np.array(colors, dtype=np.uint8)
        realImg = Image.fromarray(img.cpu().permute(1,2,0).numpy().astype(np.uint8))
        annIm = Image.fromarray(classColorMap[out])
        annImage = annIm.convert('RGB')
        overlay = Image.blend(realImg, annImage, alpha=.7)
        listOverlay.append(overlay)
    return listOverlay

overlay = mask_to_image(results, numClasses, colors=colors)

fig, axes = plt.subplots(5, 5, figsize=(80,80))
indices = np.arange(0, len(overlay), step=1)
for i, ax in enumerate(axes.flatten()):
    ax.imshow(overlay[random.choices(indices)[0]])
    ax.axis('off')

plt.show()